{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ETwEl6Ztb0u1",
        "Ga2yWuYocdZN",
        "CBnU9f6bdlzB",
        "Zh8ii8rBUf_e",
        "S_QBTbOVeo0D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardtz12/Pokemon-Battle-Game/blob/master/final_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6r5T3CVUhWV",
        "colab_type": "text"
      },
      "source": [
        "# Final Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s63PC_c8bNul",
        "colab_type": "text"
      },
      "source": [
        "##1.0 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETwEl6Ztb0u1",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Show me the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vMcZzAJUn3P",
        "colab_type": "code",
        "outputId": "1b9979e6-868c-477b-9d72-cf6dfa0924ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  1 04:15:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7etO4D-hb7L0",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 download the packages I need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_baDb1wwbgpW",
        "colab_type": "code",
        "outputId": "d59fe0c2-8475-495e-cf22-0fe53dcd84c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# download \n",
        "!pip install transformers\n",
        "!pip install tensorboardX\n",
        "!pip install seaborn\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install transformers\n",
        "!pip install captum\n",
        "!pip install beautifulsoup4\n",
        "!pip install nltk\n",
        "!pip install google-api-python-client\n",
        "!pip install Google-Search-API==1.1.14"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.18.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.0.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from captum) (1.18.3)\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from captum) (1.5.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from captum) (3.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->captum) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->captum) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->captum) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->captum) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->captum) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.12.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (1.7.12)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.17.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.7.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.12.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (46.1.3)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.8)\n",
            "Requirement already satisfied: Google-Search-API==1.1.14 in /usr/local/lib/python3.6/dist-packages (1.1.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (4.6.3)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (1.1.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (0.1.11)\n",
            "Requirement already satisfied: selenium<3.0.0,>=2.44.0 in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (2.53.6)\n",
            "Requirement already satisfied: vcrpy in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (4.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from Google-Search-API==1.1.14) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Google-Search-API==1.1.14) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Google-Search-API==1.1.14) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->Google-Search-API==1.1.14) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->Google-Search-API==1.1.14) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from vcrpy->Google-Search-API==1.1.14) (1.12.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from vcrpy->Google-Search-API==1.1.14) (3.13)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from vcrpy->Google-Search-API==1.1.14) (1.12.1)\n",
            "Requirement already satisfied: yarl; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from vcrpy->Google-Search-API==1.1.14) (1.4.2)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.6/dist-packages (from yarl; python_version >= \"3.6\"->vcrpy->Google-Search-API==1.1.14) (4.7.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLxkRklPgc72",
        "colab_type": "code",
        "outputId": "e2361e1b-155c-46ab-e6b2-3db6facf96df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import random as r\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga2yWuYocdZN",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 check CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bFKV2HIciyt",
        "colab_type": "code",
        "outputId": "08b369bf-70fc-46e1-cb74-cda411c0d4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8N7sg9SbwPu",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Retrieving Passages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm9wReomc2BO",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 helper functions to process and return links / passages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKN_x5MicNuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googlesearch import search\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "import urllib.request\n",
        "import requests\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "\"\"\"\n",
        "helper function to clean out the question\n",
        "1. lower case\n",
        "2. remove punctuation\n",
        "\n",
        "NOTE: i added removing stop words thing,\n",
        "but the questions get destroyed with it:\n",
        "\"who is the director of WHO\" becomes just \"director\"\n",
        "\"\"\"\n",
        "def processQuestion(question):\n",
        "    # convert to lower case, word list\n",
        "    word_tokens = word_tokenize(question.lower())\n",
        "    \n",
        "    # remove punctuation from each word\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    word_tokens = [w.translate(table) for w in word_tokens]\n",
        "            \n",
        "    # return back the sentence\n",
        "    return (\" \").join(word for word in word_tokens)\n",
        "\n",
        "\"\"\"\n",
        "get search result links from google\n",
        "\"\"\"\n",
        "def getLinksFromGoogle(q, num):\n",
        "  my_results_list = []\n",
        "  for i in search(q,        # The query you want to run\n",
        "                lang = 'en',  # The language\n",
        "                num = 10,     # Number of results per page\n",
        "                start = 0,    # First result to retrieve\n",
        "                stop = num,  # Last result to retrieve\n",
        "                pause = 3.0,  # Lapse between HTTP requests\n",
        "               ):\n",
        "    my_results_list.append(i)\n",
        "  return my_results_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "helper function that returns [String] paragraph(s) based on given URL of a wikipedia page\n",
        "# TODO:-// clean out special characters  \n",
        "\"\"\"\n",
        "def getWikiText(link):\n",
        "    webpage = urllib.request.urlopen(link).read()\n",
        "    soup = BeautifulSoup(webpage, 'html.parser')\n",
        "    if len(soup.findAll(\"div\",{\"id\": \"toc\"})) == 0:\n",
        "      a = soup.find('p').text\n",
        "      text_list = []\n",
        "      temp = a.replace('\\n', '')   # remove new line characters\n",
        "      temp = re.sub('\\[[0-9]{1,3}\\]', '', temp)  # remove referece numbers\n",
        "      \n",
        "      # remove special characters, downside: all special characters are gone (some maybe used in names)\n",
        "      temp =  \"\".join([x if ord(x) < 128 else ' ' for x in temp])\n",
        "      \n",
        "      # Another way to remove special characters but can't replace with a space\n",
        "      # temp = item.text.encode('ascii', 'ignore').decode('utf-8')    \n",
        "          \n",
        "      if temp:  # some item might be empty\n",
        "          text_list.append(temp)\n",
        "      return text_list\n",
        "    else:\n",
        "      a = soup.findAll(\"div\", {\"id\": \"toc\"})[0].find_all_previous('p', recursive=False)\n",
        "      a.reverse()\n",
        "      text_list = []\n",
        "      for item in a:\n",
        "          temp = item.text.replace('\\n', '')   # remove new line characters\n",
        "          temp = re.sub('\\[[0-9]{1,3}\\]', '', temp)  # remove referece numbers\n",
        "          \n",
        "          # remove special characters, downside: all special characters are gone (some maybe used in names)\n",
        "          temp =  \"\".join([x if ord(x) < 128 else ' ' for x in temp])\n",
        "          \n",
        "          # Another way to remove special characters but can't replace with a space\n",
        "          # temp = item.text.encode('ascii', 'ignore').decode('utf-8')    \n",
        "              \n",
        "          if temp:  # some item might be empty\n",
        "              text_list.append(temp)\n",
        "      return text_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "helper function that returns [String] paragraph(s) based on given URL of a non-wikipedia page\n",
        "\"\"\"\n",
        "def getNonWikiText(link):\n",
        "    webpage = urllib.request.Request(link, headers={'User-Agent': 'Chrome/80.0.3987.87'})\n",
        "    page = urllib.request.urlopen(webpage).read()\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    a = soup.findAll('p')\n",
        "    text_list = []\n",
        "    count = 0\n",
        "    num = min(len(a), 3)  # number of paragraph limits\n",
        "    for item in a:\n",
        "        temp = item.text.replace('\\n', '')   # remove new line characters\n",
        "        temp =  \"\".join([x if ord(x) < 128 else ' ' for x in temp])     # remove special characters\n",
        "        if temp:  # some item may be empty\n",
        "            text_list.append(temp)\n",
        "            count += 1\n",
        "            if (count >= num):\n",
        "                break\n",
        "    return text_list\n",
        "  \n",
        "def parse(string):\n",
        "    \"\"\"\n",
        "    Parse a paragraph. Devide it into sentences and try to generate quesstions from each sentences.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        txt = TextBlob(string)\n",
        "        # Each sentence is taken from the string input and passed to genQuestion() to generate questions.\n",
        "        questionArr = []\n",
        "        for sentence in txt.sentences:\n",
        "          newQuestion = genQuestion(sentence)\n",
        "          if newQuestion is not None:\n",
        "            questionArr.append(newQuestion)\n",
        "        return questionArr\n",
        "\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n",
        "def genQuestion(line):\n",
        "    \"\"\"\n",
        "    outputs question from the given text\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    if type(line) is str:     # If the passed variable is of type string.\n",
        "        line = TextBlob(line) # Create object of type textblob.blob.TextBlob\n",
        "\n",
        "    bucket = {}               # Create an empty dictionary\n",
        "\n",
        "\n",
        "    for i,j in enumerate(line.tags):  # line.tags are the parts-of-speach in English\n",
        "        if j[1] not in bucket:\n",
        "            bucket[j[1]] = i  # Add all tags to the dictionary or bucket variable\n",
        "    \n",
        "    # if verbose:               # In verbose more print the key,values of dictionary\n",
        "    \n",
        "    question = ''            # Create an empty string \n",
        "\n",
        "    # These are the english part-of-speach tags used in this demo program.\n",
        "    #.....................................................................\n",
        "    # NNS     Noun, plural\n",
        "    # JJ  Adjective \n",
        "    # NNP     Proper noun, singular \n",
        "    # VBG     Verb, gerund or present participle \n",
        "    # VBN     Verb, past participle \n",
        "    # VBZ     Verb, 3rd person singular present \n",
        "    # VBD     Verb, past tense \n",
        "    # IN      Preposition or subordinating conjunction \n",
        "    # PRP     Personal pronoun \n",
        "    # NN  Noun, singular or mass \n",
        "    #.....................................................................\n",
        "\n",
        "    # Create a list of tag-combination\n",
        "\n",
        "    l1 = ['NNP', 'VBG', 'VBZ', 'IN']\n",
        "    l2 = ['NNP', 'VBG', 'VBZ']\n",
        "    \n",
        "\n",
        "    l3 = ['PRP', 'VBG', 'VBZ', 'IN']\n",
        "    l4 = ['PRP', 'VBG', 'VBZ']\n",
        "    l5 = ['PRP', 'VBG', 'VBD']\n",
        "    l6 = ['NNP', 'VBG', 'VBD']\n",
        "    l7 = ['NN', 'VBG', 'VBZ']\n",
        "\n",
        "    l8 = ['NNP', 'VBZ', 'JJ']\n",
        "    l9 = ['NNP', 'VBZ', 'NN']\n",
        "\n",
        "    l10 = ['NNP', 'VBZ']\n",
        "    l11 = ['PRP', 'VBZ']\n",
        "    l12 = ['NNP', 'NN', 'IN']\n",
        "    l13 = ['NN', 'VBZ']\n",
        "\n",
        "\n",
        "    # With the use of conditional statements the dictionary is compared with the list created above\n",
        "\n",
        "    \n",
        "    if all(key in  bucket for key in l1): #'NNP', 'VBG', 'VBZ', 'IN' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    \n",
        "    elif all(key in  bucket for key in l2): #'NNP', 'VBG', 'VBZ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NNP']] +' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    \n",
        "    elif all(key in  bucket for key in l3): #'PRP', 'VBG', 'VBZ', 'IN' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['PRP']]+ ' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    \n",
        "    elif all(key in  bucket for key in l4): #'PRP', 'VBG', 'VBZ' in sentence.\n",
        "        question = 'What ' + line.words[bucket['PRP']] +' '+  ' does ' + line.words[bucket['VBG']]+ ' '+  line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    elif all(key in  bucket for key in l7): #'NN', 'VBG', 'VBZ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] +' '+ line.words[bucket['NN']] +' '+ line.words[bucket['VBG']] + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l8): #'NNP', 'VBZ', 'JJ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l9): #'NNP', 'VBZ', 'NN' in sentence\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NNP']] + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l11): #'PRP', 'VBZ' in sentence.\n",
        "        if line.words[bucket['PRP']] in ['she','he']:\n",
        "            question = 'What' + ' does ' + line.words[bucket['PRP']].lower() + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l10): #'NNP', 'VBZ' in sentence.\n",
        "        question = 'What' + ' does ' + line.words[bucket['NNP']] + ' ' + line.words[bucket['VBZ']].singularize() + '?'\n",
        "\n",
        "    elif all(key in bucket for key in l13): #'NN', 'VBZ' in sentence.\n",
        "        question = 'What' + ' ' + line.words[bucket['VBZ']] + ' ' + line.words[bucket['NN']] + '?'\n",
        "\n",
        "    # When the tags are generated 's is split to ' and s. To overcome this issue.\n",
        "    if 'VBZ' in bucket and line.words[bucket['VBZ']] == \"’\":\n",
        "        question = question.replace(\" ’ \",\"'s \")\n",
        "\n",
        "    # Print the genetated questions as output.\n",
        "    if question != '' and question is not None:\n",
        "        return question\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBnU9f6bdlzB",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 process question + retreive context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh90BJc4dtbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "main function. given a input_q, \n",
        "returns back processed input_q and a list\n",
        "of paragraphs that should have the answer.\n",
        "\"\"\"\n",
        "def get_q_plus_paragrpaghs(input_q):\n",
        "    #1. pre-process the question\n",
        "    input_q = processQuestion(input_q)\n",
        "    \n",
        "    # 2. get at-max 10 links from wiki results\n",
        "    links_wiki = getLinksFromGoogle(\"site:wikipedia.org \" + input_q, 10)\n",
        "\n",
        "    # get at-max 10 links from non-wiki/social media results\n",
        "    links_nonwiki = getLinksFromGoogle(\n",
        "            \"-site:wikipedia.org -site:twitter.com -site:instagram.com -site:youtube.com -site:spotify.com \" + input_q, 10)\n",
        "    \n",
        "    #3. get top paragraphs from each of these links\n",
        "    sentences = []\n",
        "    \n",
        "    # extract wiki paragraphs before content list\n",
        "    l = [links_wiki.index(i) for i in links_wiki if 'wikipedia.org' in i] # getting the indice of all links which are wikipedia\n",
        "    if len(l) > 0: # checking if more than one such indice exists\n",
        "        sentences.extend(getWikiText(links_wiki[l[0]])) # get the data from the 1st wiki link // TODO??\n",
        "    \n",
        "    # extract 3 paragraphs from non-wiki link\n",
        "    if len(links_nonwiki) > 0: # checking if a non-wiki link exists\n",
        "      sentences.extend(getNonWikiText(links_nonwiki[0])) # pulling content from the 1st non-wiki link // TODO??\n",
        "    \n",
        "    return input_q, sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh8ii8rBUf_e",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Setup Visualizations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asWy5-3AUiuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "948625d0-d37e-4d6c-8933-f4da0fdd19fd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def show_vis(start_scores, end_scores, tokens):\n",
        "\n",
        "  # Use plot styling from seaborn.\n",
        "  sns.set(style='darkgrid')\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  #sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"] = (16,8)\n",
        "\n",
        "  # Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
        "  s_scores = start_scores.detach().numpy().flatten()\n",
        "  e_scores = end_scores.detach().numpy().flatten()\n",
        "\n",
        "  # We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
        "  # to be unique, so we'll add the token index to the end of each one.\n",
        "  token_labels = []\n",
        "  for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))\n",
        "\n",
        "  # Create a barplot showing the start word score for all of the tokens.\n",
        "  ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
        "\n",
        "  # Turn the xlabels vertical.\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "  # Turn on the vertical grid to help align words to scores.\n",
        "  ax.grid(True)\n",
        "\n",
        "  plt.title('Start Word Scores')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # Create a barplot showing the end word score for all of the tokens.\n",
        "  ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n",
        "\n",
        "  # Turn the xlabels vertical.\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "\n",
        "  # Turn on the vertical grid to help align words to scores.\n",
        "  ax.grid(True)\n",
        "\n",
        "  plt.title('End Word Scores')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_QBTbOVeo0D",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 get predictions using pre-trained ALBERT model on SQuAD dataset\n",
        "\n",
        "training code [here](https://colab.research.google.com/drive/1P4lUkvxCzv4m8fmxHU3wGwMYYgFA7f0w#scrollTo=frTeTcy4WdbY) on `ALBERT finetuning.ipynb`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yeM-V9li_T8",
        "colab_type": "code",
        "outputId": "53ca53d3-e824-4f3c-dfb7-30c96b06df01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# get the model_ouput (all checkpoints) from GCS bucket\n",
        "!wget https://storage.googleapis.com/ica_2020/model_output.zip\n",
        "!unzip model_output.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-01 04:16:00--  https://storage.googleapis.com/ica_2020/model_output.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 2a00:1450:4013:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2027831147 (1.9G) [application/zip]\n",
            "Saving to: ‘model_output.zip’\n",
            "\n",
            "model_output.zip    100%[===================>]   1.89G  78.9MB/s    in 25s     \n",
            "\n",
            "2020-05-01 04:16:25 (78.2 MB/s) - ‘model_output.zip’ saved [2027831147/2027831147]\n",
            "\n",
            "Archive:  model_output.zip\n",
            "   creating: content/model_output/\n",
            "   creating: content/model_output/checkpoint-4000/\n",
            "  inflating: content/model_output/checkpoint-4000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-4000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-4000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-4000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-4000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-4000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-4000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-4000/config.json  \n",
            "  inflating: content/model_output/checkpoint-4000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-20000/\n",
            "  inflating: content/model_output/checkpoint-20000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-20000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-20000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-20000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-20000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-20000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-20000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-20000/config.json  \n",
            "  inflating: content/model_output/checkpoint-20000/spiece.model  \n",
            "  inflating: content/model_output/null_odds_.json  \n",
            "  inflating: content/model_output/predictions_.json  \n",
            "   creating: content/model_output/checkpoint-10000/\n",
            "  inflating: content/model_output/checkpoint-10000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-10000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-10000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-10000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-10000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-10000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-10000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-10000/config.json  \n",
            "  inflating: content/model_output/checkpoint-10000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-28000/\n",
            "  inflating: content/model_output/checkpoint-28000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-28000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-28000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-28000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-28000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-28000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-28000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-28000/config.json  \n",
            "  inflating: content/model_output/checkpoint-28000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-30000/\n",
            "  inflating: content/model_output/checkpoint-30000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-30000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-30000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-30000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-30000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-30000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-30000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-30000/config.json  \n",
            "  inflating: content/model_output/checkpoint-30000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-2000/\n",
            "  inflating: content/model_output/checkpoint-2000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-2000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-2000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-2000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-2000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-2000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-2000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-2000/config.json  \n",
            "  inflating: content/model_output/checkpoint-2000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-32000/\n",
            "  inflating: content/model_output/checkpoint-32000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-32000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-32000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-32000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-32000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-32000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-32000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-32000/config.json  \n",
            "  inflating: content/model_output/checkpoint-32000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-22000/\n",
            "  inflating: content/model_output/checkpoint-22000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-22000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-22000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-22000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-22000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-22000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-22000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-22000/config.json  \n",
            "  inflating: content/model_output/checkpoint-22000/spiece.model  \n",
            "  inflating: content/model_output/nbest_predictions_.json  \n",
            "   creating: content/model_output/checkpoint-24000/\n",
            "  inflating: content/model_output/checkpoint-24000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-24000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-24000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-24000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-24000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-24000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-24000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-24000/config.json  \n",
            "  inflating: content/model_output/checkpoint-24000/spiece.model  \n",
            "  inflating: content/model_output/tokenizer_config.json  \n",
            "   creating: content/model_output/checkpoint-12000/\n",
            "  inflating: content/model_output/checkpoint-12000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-12000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-12000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-12000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-12000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-12000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-12000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-12000/config.json  \n",
            "  inflating: content/model_output/checkpoint-12000/spiece.model  \n",
            "  inflating: content/model_output/pytorch_model.bin  \n",
            " extracting: content/model_output/added_tokens.json  \n",
            "  inflating: content/model_output/training_args.bin  \n",
            "   creating: content/model_output/checkpoint-6000/\n",
            "  inflating: content/model_output/checkpoint-6000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-6000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-6000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-6000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-6000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-6000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-6000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-6000/config.json  \n",
            "  inflating: content/model_output/checkpoint-6000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-26000/\n",
            "  inflating: content/model_output/checkpoint-26000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-26000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-26000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-26000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-26000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-26000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-26000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-26000/config.json  \n",
            "  inflating: content/model_output/checkpoint-26000/spiece.model  \n",
            "  inflating: content/model_output/special_tokens_map.json  \n",
            "   creating: content/model_output/checkpoint-16000/\n",
            "  inflating: content/model_output/checkpoint-16000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-16000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-16000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-16000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-16000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-16000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-16000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-16000/config.json  \n",
            "  inflating: content/model_output/checkpoint-16000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-18000/\n",
            "  inflating: content/model_output/checkpoint-18000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-18000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-18000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-18000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-18000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-18000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-18000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-18000/config.json  \n",
            "  inflating: content/model_output/checkpoint-18000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-14000/\n",
            "  inflating: content/model_output/checkpoint-14000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-14000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-14000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-14000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-14000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-14000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-14000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-14000/config.json  \n",
            "  inflating: content/model_output/checkpoint-14000/spiece.model  \n",
            "   creating: content/model_output/checkpoint-8000/\n",
            "  inflating: content/model_output/checkpoint-8000/optimizer.pt  \n",
            "  inflating: content/model_output/checkpoint-8000/scheduler.pt  \n",
            "  inflating: content/model_output/checkpoint-8000/tokenizer_config.json  \n",
            "  inflating: content/model_output/checkpoint-8000/pytorch_model.bin  \n",
            " extracting: content/model_output/checkpoint-8000/added_tokens.json  \n",
            "  inflating: content/model_output/checkpoint-8000/training_args.bin  \n",
            "  inflating: content/model_output/checkpoint-8000/special_tokens_map.json  \n",
            "  inflating: content/model_output/checkpoint-8000/config.json  \n",
            "  inflating: content/model_output/checkpoint-8000/spiece.model  \n",
            "  inflating: content/model_output/config.json  \n",
            "  inflating: content/model_output/spiece.model  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlHTgC5nfAKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import (\n",
        "    AlbertConfig,\n",
        "    AlbertForQuestionAnswering,\n",
        "    AlbertTokenizer,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "\n",
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "\n",
        "# READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
        "use_own_model = True\n",
        "\n",
        "if use_own_model:\n",
        "  model_name_or_path = \"content/model_output\"\n",
        "else:\n",
        "  model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
        "\n",
        "output_dir = \"\"\n",
        "\n",
        "# Config\n",
        "n_best_size = 3\n",
        "max_answer_length = 30\n",
        "do_lower_case = True\n",
        "null_score_diff_threshold = 0.0\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "# Setup model\n",
        "config_class, model_class, tokenizer_class = (\n",
        "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
        "config = config_class.from_pretrained(model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(\n",
        "    model_name_or_path, do_lower_case=True)\n",
        "model = model_class.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "processor = SquadV2Processor()\n",
        "\n",
        "def run_prediction(question_texts, context_text):\n",
        "    \"\"\"Setup function to compute predictions\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for i, question_text in enumerate(question_texts):\n",
        "        example = SquadExample(\n",
        "            qas_id=str(i),\n",
        "            question_text=question_text,\n",
        "            context_text=context_text,\n",
        "            answer_text=None,\n",
        "            start_position_character=None,\n",
        "            title=\"Predict\",\n",
        "            is_impossible=False,\n",
        "            answers=None,\n",
        "        )\n",
        "\n",
        "        examples.append(example)\n",
        "\n",
        "    features, dataset = squad_convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=384,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        return_dataset=\"pt\",\n",
        "        threads=1,\n",
        "    )\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            for i, example_index in enumerate(example_indices):\n",
        "                eval_feature = features[example_index.item()]\n",
        "                unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "                output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "                all_results.append(result)\n",
        "\n",
        "    output_prediction_file = \"predictions.json\"\n",
        "    output_nbest_file = \"nbest_predictions.json\"\n",
        "    output_null_log_odds_file = \"null_predictions.json\"\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        n_best_size,\n",
        "        max_answer_length,\n",
        "        do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        False,  # verbose_logging\n",
        "        True,  # version_2_with_negative\n",
        "        null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqGB4vPXfAet",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-IxwzxKflwm",
        "colab_type": "code",
        "outputId": "0da49f4c-97d4-47d5-f8cb-a7e4efd07cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "context = \"The Georgia Institute of Technology, commonly referred to as Georgia Tech or, in the state of Georgia, as Tech,[8] is a public research university and institute of technology in Atlanta, Georgia.[9] It is part of the University System of Georgia and has satellite campuses in Savannah, Georgia; Metz, France; Athlone, Ireland; Shenzhen, China; and Singapore. The school was founded in 1885 as the Georgia School of Technology as part of Reconstruction plans to build an industrial economy in the post-Civil War Southern United States. Initially, it offered only a degree in mechanical engineering. By 1901, its curriculum had expanded to include electrical, civil, and chemical engineering. In 1948, the school changed its name to reflect its evolution from a trade school to a larger and more capable technical institute and research universi\"\n",
        "questions = [\"where is georgia tech?\"]\n",
        "\n",
        "# Run method\n",
        "predictions = run_prediction(questions, context)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 143.40it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 6482.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Atlanta, Georgia.[9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wApSWueIfqHD",
        "colab_type": "code",
        "outputId": "fb7715d4-6650-4c24-99f7-2e3f0fb602cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"Who is the secretary general of united nations?\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 145.40it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 3182.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The secretary-general serves as the chief administrative officer of the United Nations.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnJC8nCrR9HR",
        "colab_type": "code",
        "outputId": "ee3b8315-47bd-464e-cd5a-650ea5707482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"where is georgia tech\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 107.92it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2353.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "is a public research university and institute of technology in Atlanta, Georgia.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJo3-x8LWjIm",
        "colab_type": "code",
        "outputId": "d498e89f-f051-4ef7-8d76-85258703d4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"what is stack overflow\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 135.00it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2261.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "if the call stack pointer exceeds the stack bound\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydu5aDiUWmhr",
        "colab_type": "code",
        "outputId": "67b3e8e7-c077-470c-d519-8878e9c8d5be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"what is github\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 176.74it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 368.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GitHub, Inc. is a United States-based global company that provides hosting for software development version control using Git.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6H3WdDWpAU",
        "colab_type": "code",
        "outputId": "a9f66a77-6ec8-4e7c-be5f-d52802687db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"who is the founder of slack\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 103.57it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2132.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Daniel Stewart Butterfield\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_cQeteUWrCm",
        "colab_type": "code",
        "outputId": "e9086df8-7f15-477f-fdc1-7495f8902125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"who is the founder of facebook\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 102.38it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 3603.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mark Elliot Zuckerberg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIbw4fqOWtUQ",
        "colab_type": "code",
        "outputId": "0c2215fd-c0ff-4ab8-8047-52131241e2ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"who made python\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 110.23it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2024.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Guido van Rossum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bit_m8v4Wupm",
        "colab_type": "code",
        "outputId": "f10afb84-e72c-4301-ce46-16b34983a584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "input_q, paras = get_q_plus_paragrpaghs(\"who is george p burdell\")\n",
        "paras = ''.join(paras)\n",
        "if len(paras) > 400:\n",
        "  paras = paras[:400]\n",
        "predictions = run_prediction([input_q], paras)\n",
        "\n",
        "# Print results\n",
        "for key in predictions.keys():\n",
        "  print(predictions[key])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 157.81it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 6875.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "George P. Burdell is a fictitious student officially enrolled at Georgia Tech in 1927 as a practical joke.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUmpN-1qpkrt",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 Interactive example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0du6DA6SqFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "ebd78697-cf15-4988-95a5-bd019b3f220e"
      },
      "source": [
        "while True:\n",
        "    inputt = input(\"YOU: \")\n",
        "    if inputt == \"END\":\n",
        "        break\n",
        "    else:\n",
        "        lastPeriod = 0\n",
        "        for i in range(len(inputt)):\n",
        "          if inputt[i] == \".\" or inputt[i] == \"!\":\n",
        "            lastPeriod = i\n",
        "        inputt = inputt[lastPeriod:]\n",
        "        input_q, paras = get_q_plus_paragrpaghs(inputt)\n",
        "        paras = ''.join(paras)\n",
        "        if len(paras) > 400:\n",
        "          paras = paras[:400]\n",
        "        predictions = run_prediction([input_q], paras)\n",
        "        for key in predictions.keys():\n",
        "          if predictions[key] is None or predictions[key] == \"\":\n",
        "            print(\"Sorry I don't know the answer to that question! Maybe try asking something else?\")\n",
        "          else:\n",
        "            out = \"ICA: {}\".format(predictions[key])\n",
        "            questionArr = parse(paras)\n",
        "            if len(questionArr) > 0:\n",
        "              randNum = r.randint(0, len(questionArr) - 1)\n",
        "              print (out + \". \" + questionArr[randNum])\n",
        "            else:\n",
        "              print (out + \". \")\n",
        "\n",
        "#what is a coronavirus?\n",
        "#what is COVID-19?\n",
        "#what is death?\n",
        "#what is the meaning of life?"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "YOU: What is the meaning of life?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 161.38it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 6364.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ICA: The meaning of life. What is life?\n",
            "YOU: Life is just anything that grows and reproduces I guess. Is there life that lies beyond Earth?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 144.65it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2978.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sorry I don't know the answer to that question! Maybe try asking something else?\n",
            "YOU: Are there aliens?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 158.70it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 6875.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sorry I don't know the answer to that question! Maybe try asking something else?\n",
            "YOU: Who is the president of Georgia Tech?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 488.11it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2003.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sorry I don't know the answer to that question! Maybe try asking something else?\n",
            "YOU: Where is Georgia Tech?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 99.12it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 6955.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ICA: is a public research university and institute of technology in Atlanta, Georgia.. What is University?\n",
            "YOU: How do I pick up girls?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 145.73it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 2621.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ICA: How to Pick Up Girls. What is New starring?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-af2fc51c48dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minputt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YOU: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minputt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"END\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh7OPGLrpvtr",
        "colab_type": "text"
      },
      "source": [
        "# 3.0 Generate Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRSVCBkiwa8E",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5U1jAVWwbIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}